{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30aff04e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T05:39:37.541096Z",
     "start_time": "2022-09-17T05:39:37.525487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b1c2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-17T05:40:26.968171Z",
     "start_time": "2022-09-17T05:40:25.969225Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from src.data_prep import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6efa0",
   "metadata": {},
   "source": [
    "## Custom data creation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae6b551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T05:19:38.173824Z",
     "start_time": "2022-07-13T05:19:28.060222Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34835/34835 [1:03:49<00:00,  9.10it/s]\n",
      "100%|███████████████████████████████████████| 2112/2112 [01:47<00:00, 19.70it/s]\n",
      "100%|█████████████████████████████████████████| 741/741 [00:26<00:00, 27.60it/s]\n"
     ]
    }
   ],
   "source": [
    "lab_gen('train')\n",
    "lab_gen('test')\n",
    "lab_gen('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f7d1a",
   "metadata": {},
   "source": [
    "## Run testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --batch-size 8 --img-size [480, 480] --data data/custom.yaml --cfg cfg/training/yolov7.yaml --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb8dfc",
   "metadata": {},
   "source": [
    "## Testing Detect funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8c53dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:54:29.487428Z",
     "start_time": "2022-10-18T08:54:29.316370Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_labels(img,line_thickness, color, tot_labs):\n",
    "    \n",
    "    ## get image dim\n",
    "    im_ht, im_wd,_ = img.shape\n",
    "\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    #c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    max_lab_len = max([cv2.getTextSize(lab, 0, fontScale=tl / 3, thickness=tf)[0][0] for lab in tot_labs]+[cv2.getTextSize('Amenties:', 0, fontScale=tl / 3, thickness=tf)[0][0]])\n",
    "\n",
    "    x_pos = im_wd-(max_lab_len+5)\n",
    "    y_pos = 10\n",
    "\n",
    "    ## Add Amentities \n",
    "    t_size = cv2.getTextSize('Amenties:', 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    cv2.putText(img, 'Amenties:', (x_pos,y_pos), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    y_pos+=t_size[1]\n",
    "    \n",
    "\n",
    "    for lab in tot_labs:\n",
    "        t_size = cv2.getTextSize(lab, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        cv2.putText(img, lab, (x_pos, y_pos+5), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "        y_pos+=t_size[1]\n",
    "        \n",
    "def detect(opt,save_img=False):\n",
    "\n",
    "    source, weights, view_img, save_txt, imgsz, trace = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace\n",
    "    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "    # Directories\n",
    "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "\n",
    "    if trace:\n",
    "        model = TracedModel(model, device, opt.img_size)\n",
    "\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Get names and colors\n",
    "    #set_trace()\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "    ## filtered amenties labels for whole\n",
    "    if opt.filter_labels:\n",
    "        filter_labels = open(opt.filter_labels).read().splitlines()\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    t0 = time.time()\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms) ## Cahnge conf_thres and iou_thres\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Apply Classifier\n",
    "        if classify:\n",
    "            pred = apply_classifier(pred, modelc, img, im0s)\n",
    "            \n",
    "        tot_labs=[]\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            \n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "            else:\n",
    "                p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # img.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "                    \n",
    "                frame_labs=set([names[int(lb_int)] for lb_int in det[:,-1] if names[int(lb_int)] in filter_labels])\n",
    "                for lb in frame_labs: \n",
    "                    if lb not in tot_labs: tot_labs.append(lb)\n",
    "                \n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img or view_img:  # Add bbox to image\n",
    "                        label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                        if opt.filter_labels:\n",
    "                            if (names[int(cls)] in filter_labels):\n",
    "                                plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "                        else:\n",
    "                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "               \n",
    "                plot_labels(im0,3, None, tot_labs)\n",
    "            # Print time (inference + NMS)\n",
    "            #print(f'{s}Done. ({t2 - t1:.3f}s)')\n",
    "\n",
    "            # Stream results\n",
    "            if view_img:\n",
    "                cv2.imshow(str(p), im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                    print(f\" The image with the result is saved in: {save_path}\")\n",
    "                else:  # 'video' or 'stream'\n",
    "\n",
    "                    if vid_path != save_path:  # new video\n",
    "                        vid_path = save_path\n",
    "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                            vid_writer.release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                            save_path += '.mp4'\n",
    "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer.write(im0)\n",
    "\n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        #print(f\"Results saved to {save_dir}{s}\")\n",
    "\n",
    "    print(f'Done. ({time.time() - t0:.3f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7df3304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:33:23.102128Z",
     "start_time": "2022-10-18T08:33:23.098573Z"
    }
   },
   "outputs": [],
   "source": [
    "arg_dict = {\n",
    "    'filter_labels': 'Amenities_cats.txt',\n",
    "    'weights': ['yolov7.pt'],\n",
    "    'source': 'GlassCabin_AirbnbTour-cut-merged.mp4',\n",
    "    'img_size': 640,\n",
    "    'conf_thres': 0.5,\n",
    "    'iou_thres': 0.45,\n",
    "    'device': '',\n",
    "    'view_img': False,\n",
    "    'save_txt': False,\n",
    "    'save_conf': False,\n",
    "    'nosave': False,\n",
    "    'classes': None,\n",
    "    'agnostic_nms': False,\n",
    "    'augment': False,\n",
    "    'update': False,\n",
    "    'project': 'runs/detect',\n",
    "    'name': 'exp',\n",
    "    'exist_ok': False,\n",
    "    'no_trace': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf34acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:33:58.225479Z",
     "start_time": "2022-10-18T08:33:58.222401Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('yolov7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad559a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:34:01.244893Z",
     "start_time": "2022-10-18T08:34:00.295407Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "from pdb import set_trace\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# add some arguments\n",
    "# add the other arguments\n",
    "for k, v in arg_dict.items():\n",
    "    parser.add_argument('--' + k, default=v)\n",
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f138765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:54:42.038384Z",
     "start_time": "2022-10-18T08:54:33.300583Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR 🚀 v0.1-37-g987dd83 torch 1.12.1+cu102 CUDA:0 (NVIDIA GeForce GTX 1660, 5941.625MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n",
      "/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "video 1/1 (1/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (2/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (3/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (4/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (5/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (6/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (7/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (8/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (9/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (10/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (11/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (12/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: video 1/1 (13/6452) /home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/GlassCabin_AirbnbTour-cut-merged.mp4: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/models/common.py(280): forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/models/yolo.py(345): forward_once\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/models/yolo.py(319): forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/jit/_trace.py(974): trace_module\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/jit/_trace.py(759): trace\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/utils/torch_utils.py(362): __init__\n/tmp/ipykernel_28557/1407345556.py(50): detect\n/tmp/ipykernel_28557/1533378750.py(1): <module>\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3457): run_code\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3377): run_ast_nodes\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3186): run_cell_async\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2960): _run_cell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2915): run_cell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/zmqshell.py(532): run_cell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/ipkernel.py(360): do_execute\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(662): execute_request\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(367): dispatch_shell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(460): process_one\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(471): dispatch_queue\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/asyncio/events.py(88): _run\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/asyncio/base_events.py(1786): _run_once\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/asyncio/base_events.py(541): run_forever\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/tornado/platform/asyncio.py(199): start\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelapp.py(677): start\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/traitlets/config/application.py(846): launch_instance\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel_launcher.py(16): <module>\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/runpy.py(85): _run_code\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/runpy.py(193): _run_module_as_main\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 5.80 GiB total capacity; 4.47 GiB already allocated; 31.06 MiB free; 4.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28557/1533378750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28557/1407345556.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(opt, save_img)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_synchronized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Apply NMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/utils/torch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, profile)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/models/common.py(280): forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/models/yolo.py(345): forward_once\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/models/yolo.py(319): forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1118): _slow_forward\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/nn/modules/module.py(1130): _call_impl\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/jit/_trace.py(974): trace_module\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/torch/jit/_trace.py(759): trace\n/home/skumar/DataScience/Projects_Section/Projects_Working/Yolo_OjbDetection/yolov7/utils/torch_utils.py(362): __init__\n/tmp/ipykernel_28557/1407345556.py(50): detect\n/tmp/ipykernel_28557/1533378750.py(1): <module>\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3457): run_code\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3377): run_ast_nodes\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(3186): run_cell_async\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/async_helpers.py(78): _pseudo_sync_runner\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2960): _run_cell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/IPython/core/interactiveshell.py(2915): run_cell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/zmqshell.py(532): run_cell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/ipkernel.py(360): do_execute\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(662): execute_request\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(367): dispatch_shell\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(460): process_one\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelbase.py(471): dispatch_queue\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/asyncio/events.py(88): _run\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/asyncio/base_events.py(1786): _run_once\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/asyncio/base_events.py(541): run_forever\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/tornado/platform/asyncio.py(199): start\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel/kernelapp.py(677): start\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/traitlets/config/application.py(846): launch_instance\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/site-packages/ipykernel_launcher.py(16): <module>\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/runpy.py(85): _run_code\n/home/skumar/miniconda3/envs/yolov7/lib/python3.7/runpy.py(193): _run_module_as_main\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 5.80 GiB total capacity; 4.47 GiB already allocated; 31.06 MiB free; 4.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "detect(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b90629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T08:35:09.727959Z",
     "start_time": "2022-10-18T08:35:09.727951Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imwrite('TMP.jpg', im0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381e88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "yolov7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
